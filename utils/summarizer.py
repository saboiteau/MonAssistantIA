import os
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Configuration du provider LLM
PROVIDER = os.getenv("LLM_PROVIDER", "openai").lower()

# Initialisation des clients selon le provider
if PROVIDER == "openai":
    import openai
    openai.api_key = os.getenv("LLM_API_KEY")
elif PROVIDER == "anthropic":
    import anthropic
    client = anthropic.Anthropic(api_key=os.getenv("LLM_API_KEY"))
elif PROVIDER == "gemini":
    import google.generativeai as genai
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

def summarize(text: str, metadata: dict) -> str:
    """
    G√©n√®re une fiche de veille au format Markdown via un LLM.

    Args:
        text (str): Le texte brut de l'article.
        metadata (dict): M√©tadonn√©es (titre, auteur, date, source).

    Returns:
        str: Le contenu Markdown de la fiche g√©n√©r√©e.
    """
    
    # Prompt syst√®me pour guider le LLM
    prompt = f"""
    Tu es mon assistant √©ditorial expert en veille technologique.
    Ta mission est de g√©n√©rer une fiche de veille structur√©e au format Markdown √† partir du texte ci-dessous.
    
    Respecte scrupuleusement ce format :
    
    # Veille : [Titre de l'article]

    - **Source** : [{metadata.get('source')}]({metadata.get('source')})
    - **Date** : {metadata.get('date')}
    - **Auteur** : {metadata.get('author')}
    - **Tags** : #Tag1 #Tag2 #Tag3 (√† d√©duire du contenu)

    ## üìù R√©sum√©
    [R√©sum√© structur√© de l'article en fran√ßais. Met en avant les points cl√©s.]

    ## üß† Analyse & Pense-b√™te
    [Ton analyse critique : pourquoi c'est important ? Quel impact pour moi ? Id√©es d'application concr√®te.]
    
    ---
    
    Texte √† analyser :
    {text[:10000]} # On tronque pour √©viter de d√©passer les limites de tokens si n√©cessaire
    """

    try:
        if PROVIDER == "openai":
            # TODO: Appel API OpenAI
            pass
        elif PROVIDER == "anthropic":
            # TODO: Appel API Anthropic
            pass
        elif PROVIDER == "gemini":
            # TODO: Appel API Gemini
            pass
        else:
            return "Erreur : Provider LLM non support√© ou mal configur√©."
            
        # Placeholder pour le moment
        return f"# Fiche g√©n√©r√©e (Simulation)\n\nContenu bas√© sur {metadata['title']}"

    except Exception as e:
        return f"Erreur lors de la g√©n√©ration du r√©sum√© : {str(e)}"
